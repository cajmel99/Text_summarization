{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-20T13:59:22.332790Z",
     "start_time": "2025-01-20T13:59:15.942467Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], enable=True)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T13:59:22.383217Z",
     "start_time": "2025-01-20T13:59:22.334667Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Num GPUs available: \", len(tf.config.list_physical_devices('GPU')))",
   "id": "8f2eed0452646518",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs available:  1\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T13:59:29.686351Z",
     "start_time": "2025-01-20T13:59:22.384754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cnn_daily_dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "print(cnn_daily_dataset)"
   ],
   "id": "533056fec54ceb7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 287113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 11490\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T13:59:40.392961Z",
     "start_time": "2025-01-20T13:59:29.688369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.DataFrame.from_dict(cnn_daily_dataset['train'])\n",
    "val_df = pd.DataFrame.from_dict(cnn_daily_dataset['validation'])\n",
    "test_df = pd.DataFrame.from_dict(cnn_daily_dataset['test'])"
   ],
   "id": "fcc278faaa10deee",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T13:59:40.535880Z",
     "start_time": "2025-01-20T13:59:40.394253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.concat([train_df, test_df, val_df]).drop(columns='id')\n",
    "df['id'] = df.index\n",
    "df = df.iloc[:, [2, 0, 1]]"
   ],
   "id": "a705f4f152d98880",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T13:59:40.556791Z",
     "start_time": "2025-01-20T13:59:40.539964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(df.head()['article'])\n",
    "df"
   ],
   "id": "72b300818b679775",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          id                                            article  \\\n",
       "0          0  LONDON, England (Reuters) -- Harry Potter star...   \n",
       "1          1  Editor's note: In our Behind the Scenes series...   \n",
       "2          2  MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...   \n",
       "3          3  WASHINGTON (CNN) -- Doctors removed five small...   \n",
       "4          4  (CNN)  -- The National Football League has ind...   \n",
       "...      ...                                                ...   \n",
       "13363  13363  It is the dream of many young children, the ch...   \n",
       "13364  13364  It’s the type of encounter that can send panic...   \n",
       "13365  13365  A group of tourists to the Bahamas enjoyed one...   \n",
       "13366  13366  Pippa Middleton bundled up against the London ...   \n",
       "13367  13367  A teacher and wrestling coach has been charged...   \n",
       "\n",
       "                                              highlights  \n",
       "0      Harry Potter star Daniel Radcliffe gets £20M f...  \n",
       "1      Mentally ill inmates in Miami are housed on th...  \n",
       "2      NEW: \"I thought I was going to die,\" driver sa...  \n",
       "3      Five small polyps found during procedure; \"non...  \n",
       "4      NEW: NFL chief, Atlanta Falcons owner critical...  \n",
       "...                                                  ...  \n",
       "13363  The town in Valley Center, San Diego, has been...  \n",
       "13364  Photographer Graham Hewer captured the jaw-dro...  \n",
       "13365  The pigs swim through the crystal clear sea an...  \n",
       "13366  Kate's sister is back in London following 'cri...  \n",
       "13367  Megan Blair Baker, 25, has been accused of sex...  \n",
       "\n",
       "[311971 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Editor's note: In our Behind the Scenes series...</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...</td>\n",
       "      <td>NEW: \"I thought I was going to die,\" driver sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>WASHINGTON (CNN) -- Doctors removed five small...</td>\n",
       "      <td>Five small polyps found during procedure; \"non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>(CNN)  -- The National Football League has ind...</td>\n",
       "      <td>NEW: NFL chief, Atlanta Falcons owner critical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13363</th>\n",
       "      <td>13363</td>\n",
       "      <td>It is the dream of many young children, the ch...</td>\n",
       "      <td>The town in Valley Center, San Diego, has been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13364</th>\n",
       "      <td>13364</td>\n",
       "      <td>It’s the type of encounter that can send panic...</td>\n",
       "      <td>Photographer Graham Hewer captured the jaw-dro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13365</th>\n",
       "      <td>13365</td>\n",
       "      <td>A group of tourists to the Bahamas enjoyed one...</td>\n",
       "      <td>The pigs swim through the crystal clear sea an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13366</th>\n",
       "      <td>13366</td>\n",
       "      <td>Pippa Middleton bundled up against the London ...</td>\n",
       "      <td>Kate's sister is back in London following 'cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13367</th>\n",
       "      <td>13367</td>\n",
       "      <td>A teacher and wrestling coach has been charged...</td>\n",
       "      <td>Megan Blair Baker, 25, has been accused of sex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311971 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T13:59:40.767505Z",
     "start_time": "2025-01-20T13:59:40.558593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['sent_count'] = df['article'].apply(len)\n",
    "print(f'Min number of sentences: {df[\"sent_count\"].min()}, Max num of sent.: {df[\"sent_count\"].max()}')\n",
    "df = df.drop(columns='sent_count')"
   ],
   "id": "a2855afec2fd2091",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of sentences: 48, Max num of sent.: 15925\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extractive phase",
   "id": "db6e18e1910d2fad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T13:59:42.710997Z",
     "start_time": "2025-01-20T13:59:40.769808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import networkx as nx\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "id": "835d298f1b4d09a9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T13:59:43.865511Z",
     "start_time": "2025-01-20T13:59:42.712394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def extract_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    sents = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "    return sents\n",
    "\n",
    "\n",
    "def get_similarity_matrix(sentences):\n",
    "    tf_idf = TfidfVectorizer().fit_transform(sentences)\n",
    "    similarity_matrix = cosine_similarity(tf_idf)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def get_graph(sim_matrix):\n",
    "    nsents = len(sim_matrix)\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    for i in range(nsents):\n",
    "        for j in range(i + 1, nsents):\n",
    "            if sim_matrix[i, j] > 0:\n",
    "                graph.add_edge(i, j, weight=sim_matrix[i, j])\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_n_best_sents(text, nsents: int):\n",
    "    sentences = extract_sentences(text)\n",
    "    similarity_matrix = get_similarity_matrix(sentences)\n",
    "    graph = get_graph(similarity_matrix)\n",
    "    scores = nx.pagerank(graph, weight='weight')\n",
    "\n",
    "    ranked_sentences = sorted(\n",
    "        [(scores[i] if i in scores else 0, s) for i, s in enumerate(sentences)],\n",
    "        reverse=True\n",
    "    )\n",
    "    top_sentences = [s for _, s in ranked_sentences[:nsents]]\n",
    "\n",
    "    return top_sentences\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # print(str(df.head()['article'].values))\n",
    "    top_sents = get_n_best_sents(str(df.head()['article'].values), 3)\n",
    "    print(top_sents)"
   ],
   "id": "f7dbaecfe53289de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In an additional summary of facts, signed by Vick and filed with the agreement, Vick admitted buying pit bulls and the property used for training and fighting the dogs, but the statement said he did not bet on the fights or receive any of the money won.', 'His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.', 'They were in the middle of the Mississippi River, which was churning fast, and he had no way of getting to them.']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T13:59:46.668349Z",
     "start_time": "2025-01-20T13:59:43.867877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from parallel_pandas import ParallelPandas\n",
    "\n",
    "SENT_COUNT_THRESHOLD = 25\n",
    "TRIMMED_FILEPATH = 'datasets/trimmed_th_cnn_dailymail.h5'\n",
    "\n",
    "\n",
    "def process_row(sent_list):\n",
    "    half = len(sent_list) // 2\n",
    "    output_sent_count = half if half < SENT_COUNT_THRESHOLD else SENT_COUNT_THRESHOLD\n",
    "    best_sents = get_n_best_sents(sent_list, output_sent_count)\n",
    "\n",
    "    return best_sents\n",
    "\n",
    "\n",
    "def trimm_dataset(df):    \n",
    "    ParallelPandas.initialize(n_cpu=os.cpu_count() - 2, split_factor=60, disable_pr_bar=False)\n",
    "    output_df = df['article'].p_apply(lambda text: process_row(text))\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "\n",
    "trimmed = None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if os.path.exists(TRIMMED_FILEPATH):\n",
    "        print('Preprocessed dataset exists. Loading...')\n",
    "        trimmed = pd.read_hdf(TRIMMED_FILEPATH, key='dataset')\n",
    "        print('Data loaded')\n",
    "    else:\n",
    "        print('Processing a dataset...')\n",
    "        trimmed = trimm_dataset(df)\n",
    "        trimmed.to_hdf(TRIMMED_FILEPATH, key='dataset', mode='w')"
   ],
   "id": "1bdffb01dc391b5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset exists. Loading...\n",
      "Data loaded\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T22:03:21.924485Z",
     "start_time": "2025-01-19T22:03:21.059837Z"
    }
   },
   "cell_type": "code",
   "source": "# trimmed = pd.read_hdf('datasets/trimmed_th_cnn_dailymail.h5', key='dataset')",
   "id": "a870abebd7e32422",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Abstractive phase"
   ],
   "id": "a9155ee2b083eb99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T16:30:16.534652Z",
     "start_time": "2025-01-20T16:30:16.506979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Bidirectional, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# VOCAB_SZ = 5000\n",
    "# EMBEDDING_DIM = 256\n",
    "# UNITS = 256\n",
    "# MAX_SEQ_LEN = 50\n",
    "\n",
    "VOCAB_SZ = 5000\n",
    "EMBEDDING_DIM = 256\n",
    "UNITS = 256\n",
    "MAX_SEQ_LEN = 50\n",
    "\n",
    "\n",
    "def create_encoder(vocab_size, embedding_dim, units):\n",
    "    encoder_inputs = Input(shape=(MAX_SEQ_LEN,), name=\"encoder_inputs\")\n",
    "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
    "    \n",
    "    encoder_rnn = Bidirectional(\n",
    "        GRU(units, return_sequences=True, return_state=True, name=\"encoder_rnn\"),\n",
    "        name='bidirectional_encoder'\n",
    "    )\n",
    "    encoder_outputs, forward_state, backward_state = encoder_rnn(embedding)\n",
    "    last_hidden_state = tf.concat([forward_state, backward_state], axis=-1)\n",
    "    \n",
    "    return encoder_inputs, encoder_outputs, last_hidden_state\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.W_alpha = Dense(units)\n",
    "        self.U_alpha = Dense(units)\n",
    "        self.Z_alpha = Dense(1)\n",
    "\n",
    "    def call(self, decoder_state, encoder_outputs):\n",
    "        decoder_state_expanded = tf.expand_dims(decoder_state, 1)\n",
    "        alignment_scores = self.Z_alpha(\n",
    "            tf.nn.tanh(self.W_alpha(decoder_state_expanded) + self.U_alpha(encoder_outputs))\n",
    "        )\n",
    "        attention_weights = tf.nn.softmax(alignment_scores, axis=1)\n",
    "        context_vector = tf.reduce_sum(attention_weights * encoder_outputs, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class PointerDecoder(Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, units):\n",
    "        super(PointerDecoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim, name=\"decoder_embedding\")\n",
    "        self.gru = GRU(units * 2, return_sequences=True, return_state=True, name=\"decoder_rnn\")\n",
    "        self.attention = AttentionLayer(units * 2)\n",
    "        self.vocab_dense = Dense(vocab_size, activation=\"softmax\", name=\"vocab_output\")\n",
    "        self.pointer_switch = Dense(1, activation=\"sigmoid\", name=\"pointer_switch\")\n",
    "\n",
    "    def call(self, decoder_inputs, encoder_outputs, initial_state, encoder_inputs):\n",
    "        all_outputs = []\n",
    "        state = initial_state\n",
    "        embedding = self.embedding(decoder_inputs)\n",
    "\n",
    "        for t in range(MAX_SEQ_LEN):\n",
    "            context_vector, attention_weights = self.attention(state, encoder_outputs)\n",
    "\n",
    "            expanded_state = tf.expand_dims(state, axis=1)\n",
    "            pointer_logits = self.pointer_switch(tf.concat([expanded_state, tf.expand_dims(context_vector, 1)], axis=-1))\n",
    "            pointer_probs = tf.squeeze(pointer_logits, axis=-1)\n",
    "\n",
    "            decoder_input_t = tf.concat([embedding[:, t:t + 1, :], tf.expand_dims(context_vector, 1)], axis=-1)\n",
    "            output, state = self.gru(decoder_input_t, initial_state=state)\n",
    "\n",
    "            vocab_probs = self.vocab_dense(output)\n",
    "\n",
    "            encoder_inputs_int = tf.cast(encoder_inputs, tf.int32)\n",
    "            pointer_output = tf.reduce_sum(attention_weights * tf.one_hot(encoder_inputs_int, depth=self.vocab_dense.units), axis=1)\n",
    "\n",
    "            final_output = pointer_probs * pointer_output + (1 - pointer_probs) * tf.squeeze(vocab_probs, axis=1)\n",
    "            all_outputs.append(tf.expand_dims(final_output, axis=1))\n",
    "\n",
    "        decoder_outputs = tf.concat(all_outputs, axis=1)\n",
    "        \n",
    "        return decoder_outputs\n",
    "\n",
    "\n",
    "def create_model(vocab_size, embedding_dim, units):\n",
    "    encoder_inputs, encoder_outputs, encoder_last_state = create_encoder(vocab_size, embedding_dim, units)\n",
    "\n",
    "    decoder_inputs = Input(shape=(MAX_SEQ_LEN,), name=\"decoder_inputs\")\n",
    "    pointer_decoder = PointerDecoder(vocab_size, embedding_dim, units)\n",
    "    decoder_outputs = pointer_decoder(\n",
    "        decoder_inputs, encoder_outputs, encoder_last_state, encoder_inputs\n",
    "    )\n",
    "\n",
    "    model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs, name='bidirectional_encoder_decoder')\n",
    "    \n",
    "    return model"
   ],
   "id": "c77c25324d291e2",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T16:30:36.232412Z",
     "start_time": "2025-01-20T16:30:21.133931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = create_model(VOCAB_SZ, EMBEDDING_DIM, UNITS)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "id": "afb2a63c9522c576",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bidirectional_encoder_decoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder_embedding (Embedding)  (None, 50, 256)      1280000     ['encoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " bidirectional_encoder (Bidirec  [(None, 50, 512),   789504      ['encoder_embedding[0][0]']      \n",
      " tional)                         (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " tf.concat_4 (TFOpLambda)       (None, 512)          0           ['bidirectional_encoder[0][1]',  \n",
      "                                                                  'bidirectional_encoder[0][2]']  \n",
      "                                                                                                  \n",
      " pointer_decoder_4 (PointerDeco  (None, 50, 5000)    6341002     ['decoder_inputs[0][0]',         \n",
      " der)                                                             'bidirectional_encoder[0][0]',  \n",
      "                                                                  'tf.concat_4[0][0]',            \n",
      "                                                                  'encoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,410,506\n",
      "Trainable params: 8,410,506\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:53:15.740436Z",
     "start_time": "2025-01-20T15:53:15.730765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'\n",
    "\n",
    "DATASET_SZ = len(trimmed)\n",
    "TRAIN_TEST_RATIO = 0.8\n",
    "TRAIN_SZ = int(TRAIN_TEST_RATIO * DATASET_SZ)\n",
    "\n",
    "train_source_articles = trimmed[:TRAIN_SZ]\n",
    "train_target_articles = df['highlights'][:TRAIN_SZ]\n",
    "\n",
    "print(train_source_articles.shape, train_target_articles.shape)\n",
    "\n",
    "# 3 examples for testing\n",
    "test_source_articles = trimmed[TRAIN_SZ : TRAIN_SZ + 3]\n",
    "test_target_articles = df['highlights'][TRAIN_SZ : TRAIN_SZ + 3]\n",
    "\n",
    "print(test_source_articles.shape, test_target_articles.shape)"
   ],
   "id": "57104ca3e3d07820",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,) (20000,)\n",
      "(3,) (3,)\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:53:31.359497Z",
     "start_time": "2025-01-20T15:53:18.094029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer(num_words=VOCAB_SZ, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(pd.concat([train_source_articles, train_target_articles], axis=0))\n",
    "tokenizer.word_index.update({START_TOKEN: 1, END_TOKEN: 2})\n",
    "\n",
    "# Preparing training data\n",
    "encoder_input_data = tokenizer.texts_to_sequences(train_source_articles)\n",
    "decoder_input_data = tokenizer.texts_to_sequences(START_TOKEN + ' ' + train_target_articles)\n",
    "decoder_target_data = tokenizer.texts_to_sequences(train_target_articles + ' ' + END_TOKEN)\n",
    "\n",
    "encoder_input_data = pad_sequences(encoder_input_data, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "decoder_input_data = pad_sequences(decoder_input_data, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "decoder_target_data = pad_sequences(decoder_target_data, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "decoder_target_data = np.expand_dims(decoder_target_data, -1)\n",
    "\n",
    "# Preparing test data\n",
    "test_encoder_input_data = tokenizer.texts_to_sequences(test_source_articles)\n",
    "test_decoder_target_data = tokenizer.texts_to_sequences(test_target_articles + ' ' + END_TOKEN)\n",
    "\n",
    "test_encoder_input_data = pad_sequences(test_encoder_input_data, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "test_decoder_target_data = pad_sequences(test_decoder_target_data, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "print(f'encoder_input_data.shape={encoder_input_data.shape}, decoder_input_data.shape={decoder_input_data.shape}, decoder_target_data.shape={decoder_target_data.shape}\\n')\n",
    "print(f'Test data shapes -> encoder_input={test_encoder_input_data.shape}, decoder_target={test_decoder_target_data.shape}')"
   ],
   "id": "d160646eda138802",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_data.shape=(20000, 50), decoder_input_data.shape=(20000, 50), decoder_target_data.shape=(20000, 50, 1)\n",
      "\n",
      "Test data shapes -> encoder_input=(3, 50), decoder_target=(3, 50)\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T16:13:15.960740Z",
     "start_time": "2025-01-20T16:13:15.950698Z"
    }
   },
   "cell_type": "code",
   "source": "len(tokenizer.word_index)",
   "id": "cd48a0032c497fa6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125698"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T16:28:13.509332Z",
     "start_time": "2025-01-20T16:26:20.973516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=os.path.join('model', 'max_vocab_sz_e{epoch:02d}_val_loss_{val_loss:.2f}.h5'),\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=8,\n",
    "    epochs=30,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ],
   "id": "ffa890cef61a28ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 17:27:35.543146: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 208000000 exceeds 10% of free system memory.\n",
      "2025-01-20 17:27:36.487228: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 208000000 exceeds 10% of free system memory.\n",
      "2025-01-20 17:27:53.060051: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 49.59MiB (rounded to 52000000)requested by op gradient_tape/sparse_categorical_crossentropy/clip_by_value/LessEqual\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-01-20 17:27:53.061681: W tensorflow/core/common_runtime/bfc_allocator.cc:474] **************************************x**********************x****************************x*********\n",
      "2025-01-20 17:27:53.063383: W tensorflow/core/framework/op_kernel.cc:1733] RESOURCE_EXHAUSTED: failed to allocate memory\n",
      "2025-01-20 17:28:03.320057: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 49.59MiB (rounded to 52000000)requested by op gradient_tape/sparse_categorical_crossentropy/clip_by_value/GreaterEqual\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-01-20 17:28:03.320758: W tensorflow/core/common_runtime/bfc_allocator.cc:474] **************************************x**********************x****************************x*********\n",
      "2025-01-20 17:28:03.326654: W tensorflow/core/framework/op_kernel.cc:1733] RESOURCE_EXHAUSTED: failed to allocate memory\n",
      "2025-01-20 17:28:13.386956: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 198.36MiB (rounded to 208000000)requested by op sparse_categorical_crossentropy/Log\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-01-20 17:28:13.387652: W tensorflow/core/common_runtime/bfc_allocator.cc:474] **************************************x**********************x****************************x*********\n",
      "2025-01-20 17:28:13.394096: W tensorflow/core/framework/op_kernel.cc:1733] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/sparse_categorical_crossentropy/clip_by_value/LessEqual' defined at (most recent call last):\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n      result = self._run_cell(\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n      result = runner(coro)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_888/2909458884.py\", line 12, in <module>\n      history = model.fit(\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 863, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n      grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/sparse_categorical_crossentropy/clip_by_value/LessEqual'\nfailed to allocate memory\n\t [[{{node gradient_tape/sparse_categorical_crossentropy/clip_by_value/LessEqual}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_711471]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[66], line 12\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ModelCheckpoint\n\u001B[1;32m      3\u001B[0m checkpoint_callback \u001B[38;5;241m=\u001B[39m ModelCheckpoint(\n\u001B[1;32m      4\u001B[0m     filepath\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_vocab_sz_e\u001B[39m\u001B[38;5;132;01m{epoch:02d}\u001B[39;00m\u001B[38;5;124m_val_loss_\u001B[39m\u001B[38;5;132;01m{val_loss:.2f}\u001B[39;00m\u001B[38;5;124m.h5\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m      5\u001B[0m     save_weights_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     10\u001B[0m )\n\u001B[0;32m---> 12\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43mencoder_input_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder_input_data\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_target_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mcheckpoint_callback\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[1;32m     55\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mResourceExhaustedError\u001B[0m: Graph execution error:\n\nDetected at node 'gradient_tape/sparse_categorical_crossentropy/clip_by_value/LessEqual' defined at (most recent call last):\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n      result = self._run_cell(\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n      result = runner(coro)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_888/2909458884.py\", line 12, in <module>\n      history = model.fit(\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 863, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n      grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n    File \"/home/krzys/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/sparse_categorical_crossentropy/clip_by_value/LessEqual'\nfailed to allocate memory\n\t [[{{node gradient_tape/sparse_categorical_crossentropy/clip_by_value/LessEqual}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_711471]"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.9812 - accuracy: 0.7478\n",
      "Epoch 21: saving model to model/model_epoch_21_val_loss_5.73.h5\n",
      "1000/1000 [==============================] - 280s 279ms/step - loss: 0.9812 - accuracy: 0.7478 - val_loss: 5.7268 - val_accuracy: 0.3140\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.9323 - accuracy: 0.7585\n",
      "Epoch 22: saving model to model/model_epoch_22_val_loss_5.82.h5\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 0.9323 - accuracy: 0.7585 - val_loss: 5.8180 - val_accuracy: 0.3134\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.8909 - accuracy: 0.7667\n",
      "Epoch 23: saving model to model/model_epoch_23_val_loss_5.91.h5\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 0.8909 - accuracy: 0.7667 - val_loss: 5.9112 - val_accuracy: 0.3100\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.8580 - accuracy: 0.7724\n",
      "Epoch 24: saving model to model/model_epoch_24_val_loss_5.97.h5\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.8580 - accuracy: 0.7724 - val_loss: 5.9729 - val_accuracy: 0.3103\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.8272 - accuracy: 0.7788\n",
      "Epoch 25: saving model to model/model_epoch_25_val_loss_6.06.h5\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.8272 - accuracy: 0.7788 - val_loss: 6.0570 - val_accuracy: 0.3097\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.7911 - accuracy: 0.7863\n",
      "Epoch 26: saving model to model/model_epoch_26_val_loss_6.11.h5\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.7911 - accuracy: 0.7863 - val_loss: 6.1086 - val_accuracy: 0.3089\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.7670 - accuracy: 0.7917\n",
      "Epoch 27: saving model to model/model_epoch_27_val_loss_6.19.h5\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 0.7670 - accuracy: 0.7917 - val_loss: 6.1950 - val_accuracy: 0.3084\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.7446 - accuracy: 0.7960\n",
      "Epoch 28: saving model to model/model_epoch_28_val_loss_6.24.h5\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 0.7446 - accuracy: 0.7960 - val_loss: 6.2403 - val_accuracy: 0.3086\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.7233 - accuracy: 0.8000\n",
      "Epoch 29: saving model to model/model_epoch_29_val_loss_6.30.h5\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.7233 - accuracy: 0.8000 - val_loss: 6.2964 - val_accuracy: 0.3080\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6978 - accuracy: 0.8059\n",
      "Epoch 30: saving model to model/model_epoch_30_val_loss_6.38.h5\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.6978 - accuracy: 0.8059 - val_loss: 6.3795 - val_accuracy: 0.3082\n"
     ]
    }
   ],
   "execution_count": 20,
   "source": [
    "model.load_weights('model/model_epoch_20_val_loss_5.65.h5')\n",
    "history_extended = model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=16,\n",
    "    epochs=30,\n",
    "    initial_epoch=20,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ],
   "id": "6cfa819d35605985"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T16:42:06.809746Z",
     "start_time": "2025-01-20T16:41:06.072609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def summarize_text(model, tokenizer, input_text, max_len=MAX_SEQ_LEN):    \n",
    "    start_token = tokenizer.word_index[START_TOKEN]\n",
    "    end_token = tokenizer.word_index[END_TOKEN]\n",
    "    decoder_input = np.zeros((1, max_len))\n",
    "    decoder_input[0, 0] = start_token\n",
    "    \n",
    "    if len(input_text.shape) == 1:\n",
    "        input_text = np.expand_dims(input_text, axis=0)\n",
    "    \n",
    "    summary = []\n",
    "    for i in range(max_len):\n",
    "        decoder_output = model.predict([input_text, decoder_input])\n",
    "        predicted_id = tf.argmax(decoder_output[:, -1, :], axis=-1).numpy()[0]\n",
    "        \n",
    "        if predicted_id == end_token:\n",
    "            break\n",
    "            \n",
    "        summary.append(predicted_id)\n",
    "        decoder_input[0, i] = predicted_id\n",
    "    \n",
    "    decoded_summary = tokenizer.sequences_to_texts([summary])[0]\n",
    "    print(decoded_summary)\n",
    "        \n",
    "    return decoded_summary\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_inputs, test_targets, max_len=MAX_SEQ_LEN):\n",
    "    predictions = []\n",
    "    for article in tqdm(test_inputs, desc='Evaluating model on test set'):\n",
    "        summary = summarize_text(model, tokenizer, article, max_len)\n",
    "        predictions.append(summary)\n",
    "\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(predictions, test_targets, avg=True)\n",
    "    \n",
    "    return rouge_scores, predictions\n",
    "\n",
    "\n",
    "weights_path = 'model/model_epoch_30_val_loss_6.38.h5'\n",
    "model.load_weights(weights_path)\n",
    "\n",
    "rouge_scores, predictions = evaluate_model(model, tokenizer, test_encoder_input_data, test_decoder_target_data)\n",
    "\n",
    "print(f'Average ROUGE scores: {rouge_scores}')"
   ],
   "id": "90a826ef9da9c8b6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set:  33%|███▎      | 1/3 [00:36<01:13, 36.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set:  67%|██████▋   | 2/3 [00:48<00:21, 21.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set: 100%|██████████| 3/3 [01:00<00:00, 20.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[69], line 52\u001B[0m\n\u001B[1;32m     49\u001B[0m weights_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel/model_epoch_30_val_loss_6.38.h5\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     50\u001B[0m model\u001B[38;5;241m.\u001B[39mload_weights(weights_path)\n\u001B[0;32m---> 52\u001B[0m rouge_scores, predictions \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_encoder_input_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_decoder_target_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAverage ROUGE scores: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrouge_scores\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[69], line 44\u001B[0m, in \u001B[0;36mevaluate_model\u001B[0;34m(model, tokenizer, test_inputs, test_targets, max_len)\u001B[0m\n\u001B[1;32m     41\u001B[0m     predictions\u001B[38;5;241m.\u001B[39mappend(summary)\n\u001B[1;32m     43\u001B[0m rouge \u001B[38;5;241m=\u001B[39m Rouge()\n\u001B[0;32m---> 44\u001B[0m rouge_scores \u001B[38;5;241m=\u001B[39m \u001B[43mrouge\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_scores\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_targets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mavg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m rouge_scores, predictions\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/rouge/rouge.py:103\u001B[0m, in \u001B[0;36mRouge.get_scores\u001B[0;34m(self, hyps, refs, avg, ignore_empty)\u001B[0m\n\u001B[1;32m     98\u001B[0m     hyps_and_refs \u001B[38;5;241m=\u001B[39m [_ \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m hyps_and_refs\n\u001B[1;32m     99\u001B[0m                      \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(_[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    100\u001B[0m                      \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(_[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    101\u001B[0m     hyps, refs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mhyps_and_refs)\n\u001B[0;32m--> 103\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m(\u001B[38;5;28misinstance\u001B[39m(hyps, \u001B[38;5;28mtype\u001B[39m(refs)))\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m(\u001B[38;5;28mlen\u001B[39m(hyps) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(refs))\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m avg:\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "execution_count": 69
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
