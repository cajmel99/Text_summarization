{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from utils import calculate_scores, sum_metrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All together\n",
    "cnn_daily_dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(cnn_daily_dataset['train']).drop(columns='id')\n",
    "val_df = pd.DataFrame.from_dict(cnn_daily_dataset['validation']).drop(columns='id')\n",
    "test_df = pd.DataFrame.from_dict(cnn_daily_dataset['test']).drop(columns='id')\n",
    "\n",
    "#df = pd.concat([train_df,test_df, val_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of first article\n",
    "print(df['article'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.dataframe = self.dataframe.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "        self.dataframe['article'] = self.dataframe['article'].apply(self._clean_text)\n",
    "        self.dataframe['article'] = self.dataframe['article'].apply(self._cleaned_list_of_sentences)\n",
    "\n",
    "        return self.dataframe\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Remove non-alphabetic, non-digit, and non-dot characters from text.\n",
    "        \"\"\"\n",
    "        pattern = r'[^A-Za-z0-9.\\s]+'\n",
    "\n",
    "        return re.sub(pattern, '', text)\n",
    "\n",
    "    def _cleaned_list_of_sentences(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize sentences, remove stopwords, and apply stemming.\n",
    "        \"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        cleaned_sentences = []\n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence)\n",
    "            cleaned_words = [\n",
    "                self.stemmer.stem(word) for word in words if word.isalnum() and word not in self.stop_words\n",
    "            ]\n",
    "            cleaned_sentences.append(cleaned_words)\n",
    "\n",
    "        return cleaned_sentences\n",
    "\n",
    "\n",
    "class SummaryGenerator:\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        self.matrix = None\n",
    "        self.ranked_sentences = None\n",
    "\n",
    "    def create_matrix(self):\n",
    "        \"\"\"\n",
    "        Create a similarity matrix based on common words between sentences.\n",
    "        \"\"\"\n",
    "        n = len(self.sentences)\n",
    "        self.matrix = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                common_words = set(self.sentences[i]) & set(self.sentences[j])\n",
    "                self.matrix[i][j] = self.matrix[j][i] = len(common_words)\n",
    "\n",
    "        return self.matrix\n",
    "\n",
    "    def rank_sentences(self):\n",
    "        \"\"\"\n",
    "        Rank sentences based on the similarity matrix.\n",
    "        \"\"\"\n",
    "        ranking_vector = self.matrix.sum(axis=1)\n",
    "        self.ranked_sentences = sorted(enumerate(ranking_vector), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return self.ranked_sentences\n",
    "\n",
    "    def produce_summary(self, summary_threshold):\n",
    "        \"\"\"\n",
    "        Produce a summary based on the ranked sentences.\n",
    "        \"\"\"\n",
    "        summary_indices = [index for index, _ in self.ranked_sentences[:summary_threshold]]\n",
    "        summary = \" \".join([\" \".join(self.sentences[i]) for i in sorted(summary_indices)])\n",
    "\n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preprocess the articles \n",
    "preprocessor = TextPreprocessor(df)\n",
    "df = preprocessor.preprocess()\n",
    "\n",
    "# Generate summaries \n",
    "df['summary'] = None  \n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    tokenized_sentences = row['article']  \n",
    "    summarizer = SummaryGenerator(tokenized_sentences) \n",
    "    summarizer.create_matrix()  \n",
    "    summarizer.rank_sentences()  \n",
    "    summary = summarizer.produce_summary(summary_threshold=3)  \n",
    "    df.loc[index, 'summary'] = summary  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calculate_scores(df, 'summary', 'highlights')\n",
    "directory = 'Results_df'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "df.to_csv(os.path.join(directory, 'Multi_graph.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = sum_metrices(df, 'rouge_scores', results_folder='Results', file_name='Multi_edges_graph_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
