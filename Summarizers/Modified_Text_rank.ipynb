{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from utils import cleaned_list_of_sentences\n",
    "import nltk\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import log, sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All together\n",
    "cnn_daily_dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(cnn_daily_dataset['train']).drop(columns='id')\n",
    "val_df = pd.DataFrame.from_dict(cnn_daily_dataset['validation']).drop(columns='id')\n",
    "test_df = pd.DataFrame.from_dict(cnn_daily_dataset['test']).drop(columns='id')\n",
    "\n",
    "df = pd.concat([train_df,test_df, val_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified TextRank model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I could see the whole bridge as it was going down, as it was falling,\" Babineau said. Babineau told the Minneapolis Star-Tribune: \"On the way down, I thought I was dead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marysia/miniconda3/envs/myenv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/marysia/miniconda3/envs/myenv/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "class TextRankSummarizer:\n",
    "    def __init__(self, document, n):\n",
    "        \"\"\"\n",
    "        Initialize the TextRankSummarizer with a document and the number of sentences for the summary.\n",
    "        \"\"\"\n",
    "        self.document = document\n",
    "        self.n = n\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.sentences = nltk.sent_tokenize(document)\n",
    "        self.processed_sentences = self._preprocess_sentences()\n",
    "        self.isf_dict = self._compute_isf()\n",
    "        self.graph = self._build_graph()\n",
    "\n",
    "    def _preprocess_sentences(self):\n",
    "        \"\"\"\n",
    "        Preprocess sentences by tokenizing, converting to lowercase, and removing stopwords and non-alphanumeric words.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            [word for word in nltk.word_tokenize(sentence.lower()) if word.isalnum() and word not in self.stop_words]\n",
    "            for sentence in self.sentences\n",
    "        ]\n",
    "\n",
    "    def _compute_isf(self):\n",
    "        \"\"\"\n",
    "        Compute the Inverse Sentence Frequency (ISF) for all words in the processed sentences.\n",
    "        \"\"\"\n",
    "        isf_dict = {}\n",
    "        for sentence in self.processed_sentences:\n",
    "            for word in set(sentence):\n",
    "                if word not in isf_dict:\n",
    "                    isf_dict[word] = self._isf(word)\n",
    "        return isf_dict\n",
    "\n",
    "    def _isf(self, word):\n",
    "        \"\"\"\n",
    "        Compute ISF for a single word.\n",
    "        \"\"\"\n",
    "        sentence_count = sum(1 for sentence in self.processed_sentences if word in sentence)\n",
    "        total_sentences = len(self.processed_sentences)\n",
    "        return log(total_sentences / (1 + sentence_count))\n",
    "\n",
    "    def _build_graph(self):\n",
    "        \"\"\"\n",
    "        Build a graph where nodes represent sentences and edges represent ISF-modified cosine similarity.\n",
    "        \"\"\"\n",
    "        graph = nx.Graph()\n",
    "        for i, sentence1 in enumerate(self.processed_sentences):\n",
    "            for j, sentence2 in enumerate(self.processed_sentences):\n",
    "                if i != j:\n",
    "                    similarity = self._compute_similarity(sentence1, sentence2)\n",
    "                    if similarity > 0:\n",
    "                        graph.add_edge(i, j, weight=similarity)\n",
    "        return graph\n",
    "\n",
    "    def _compute_similarity(self, sentence1, sentence2):\n",
    "        \"\"\"\n",
    "        Compute the ISF-modified cosine similarity between two sentences.\n",
    "        \"\"\"\n",
    "        words1 = set(sentence1)\n",
    "        words2 = set(sentence2)\n",
    "        common_words = words1.intersection(words2)\n",
    "        \n",
    "        numerator = sum(\n",
    "            (sentence1.count(word) * sentence2.count(word) * self.isf_dict[word]**2) for word in common_words\n",
    "        )\n",
    "        denominator1 = sqrt(sum((sentence1.count(word) * self.isf_dict[word])**2 for word in words1))\n",
    "        denominator2 = sqrt(sum((sentence2.count(word) * self.isf_dict[word])**2 for word in words2))\n",
    "        \n",
    "        if denominator1 == 0 or denominator2 == 0:\n",
    "            return 0\n",
    "        return numerator / (denominator1 * denominator2)\n",
    "\n",
    "    def summarize(self):\n",
    "        \"\"\"\n",
    "        Summarize the document using the TextRank algorithm.\n",
    "        \"\"\"\n",
    "        self._prune_graph()\n",
    "        self._initialize_scores()\n",
    "        self._update_scores()\n",
    "        return self._extract_summary()\n",
    "\n",
    "    def _prune_graph(self):\n",
    "        \"\"\"\n",
    "        Remove edges with weights below the average weight.\n",
    "        \"\"\"\n",
    "        average_weight = np.mean([data['weight'] for _, _, data in self.graph.edges(data=True)])\n",
    "        edges_to_remove = [(u, v) for u, v, data in self.graph.edges(data=True) if data['weight'] < average_weight]\n",
    "        self.graph.remove_edges_from(edges_to_remove)\n",
    "\n",
    "    def _initialize_scores(self):\n",
    "        \"\"\"\n",
    "        Initialize TextRank scores for all nodes in the graph.\n",
    "        \"\"\"\n",
    "        for node in self.graph.nodes():\n",
    "            self.graph.nodes[node]['score'] = np.mean([data['weight'] for _, _, data in self.graph.edges(node, data=True)])\n",
    "\n",
    "    def _update_scores(self, damping_factor=0.15, max_iter=100):\n",
    "        \"\"\"\n",
    "        Iteratively update TextRank scores for all nodes in the graph.\n",
    "        \"\"\"\n",
    "        for _ in range(max_iter):\n",
    "            new_scores = {}\n",
    "            for node in self.graph.nodes():\n",
    "                neighbors = self.graph[node]\n",
    "                score_sum = sum(\n",
    "                    self.graph.nodes[neighbor]['score'] / self.graph.degree(neighbor) for neighbor in neighbors\n",
    "                )\n",
    "                new_scores[node] = damping_factor / len(self.graph) + (1 - damping_factor) * score_sum\n",
    "            for node in self.graph.nodes():\n",
    "                self.graph.nodes[node]['score'] = new_scores[node]\n",
    "\n",
    "    def _extract_summary(self):\n",
    "        \"\"\"\n",
    "        Extract the top-n sentences as the summary.\n",
    "        \"\"\"\n",
    "        ranked_sentences = sorted(\n",
    "            self.graph.nodes(data=True), key=lambda x: x[1]['score'], reverse=True\n",
    "        )\n",
    "        top_sentences = [self.sentences[node[0]] for node in ranked_sentences[:self.n]]\n",
    "        summary = sorted(top_sentences, key=lambda sentence: self.sentences.index(sentence))\n",
    "        return ' '.join(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marysia/miniconda3/envs/myenv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/marysia/miniconda3/envs/myenv/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "df['summary'] = None\n",
    "\n",
    "for index, row in enumerate(df['article']):\n",
    "    document_text = row\n",
    "    summarizer = TextRankSummarizer(document_text, n=2)\n",
    "    summary = summarizer.summarize()\n",
    "    df.loc[index, 'summary'] = summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
