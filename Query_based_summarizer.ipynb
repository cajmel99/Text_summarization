{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from math import log\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCambodian leader Hun Sen on Friday rejected opposition parties' demands \\nfor talks outside the country, accusing them of trying to ``internationalize'' \\nthe political crisis. Government and opposition parties have asked \\nKing Norodom Sihanouk to host a summit meeting after a series of post-election \\nnegotiations between the two opposition groups and Hun Sen's party \\nto form a new government failed. Opposition leaders Prince Norodom \\nRanariddh and Sam Rainsy, citing Hun Sen's threats to arrest opposition \\nfigures after two alleged attempts on his life, said they could not \\nnegotiate freely in Cambodia and called for talks at Sihanouk's residence \\nin Beijing. Hun Sen, however, rejected that. ``I would like to make \\nit clear that all meetings related to Cambodian affairs must be conducted \\nin the Kingdom of Cambodia,'' Hun Sen told reporters after a Cabinet \\nmeeting on Friday. ``No-one should internationalize Cambodian affairs. \\nIt is detrimental to the sovereignty of Cambodia,'' he said. Hun Sen's \\nCambodian People's Party won 64 of the 122 parliamentary seats in \\nJuly's elections, short of the two-thirds majority needed to form \\na government on its own. Ranariddh and Sam Rainsy have charged that \\nHun Sen's victory in the elections was achieved through widespread \\nfraud. They have demanded a thorough investigation into their election \\ncomplaints as a precondition for their cooperation in getting the \\nnational assembly moving and a new government formed. Hun Sen said \\non Friday that the opposition concerns over their safety in the country \\nwas ``just an excuse for them to stay abroad.'' Both Ranariddh and \\nSam Rainsy have been outside the country since parliament was ceremonially \\nopened on Sep. 24. Sam Rainsy and a number of opposition figures have \\nbeen under court investigation for a grenade attack on Hun Sen's Phnom \\nPenh residence on Sep. 7. Hun Sen was not home at the time of the \\nattack, which was followed by a police crackdown on demonstrators \\ncontesting Hun Sen's election victory. The Sam Rainsy Party, in a \\nstatement released Friday, accused Hun Sen of being ``unwilling to \\nmake any compromise'' on negotiations to break the deadlock. ``A meeting \\noutside Cambodia, as suggested by the opposition, could place all \\nparties on more equal footing,'' said the statement. ``But the ruling \\nparty refuses to negotiate unless it is able to threaten its negotiating \\npartners with arrest or worse.'' \\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR_PATH = '/Users/marysia/Downloads/archive/DUC2004_Summarization_Documents/duc2004_testdata/tasks1and2'\n",
    "EXAMPLE_FILE_PATH = '/Users/marysia/Downloads/archive/DUC2004_Summarization_Documents/duc2004_testdata/tasks1and2/duc2004_tasks1and2_docs/docs/1/D1.txt' \n",
    "\n",
    "with open(EXAMPLE_FILE_PATH, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Cambodian leader Hun Sen on Friday rejected opposition parties' demands for talks outside the country, accusing them of trying to ``internationalize'' the political crisis\",\n",
       " \" Government and opposition parties have asked King Norodom Sihanouk to host a summit meeting after a series of post-election negotiations between the two opposition groups and Hun Sen's party to form a new government failed\",\n",
       " \" Opposition leaders Prince Norodom Ranariddh and Sam Rainsy, citing Hun Sen's threats to arrest opposition figures after two alleged attempts on his life, said they could not negotiate freely in Cambodia and called for talks at Sihanouk's residence in Beijing\",\n",
       " ' Hun Sen, however, rejected that',\n",
       " \" ``I would like to make it clear that all meetings related to Cambodian affairs must be conducted in the Kingdom of Cambodia,'' Hun Sen told reporters after a Cabinet meeting on Friday\",\n",
       " ' ``No-one should internationalize Cambodian affairs',\n",
       " \" It is detrimental to the sovereignty of Cambodia,'' he said\",\n",
       " \" Hun Sen's Cambodian People's Party won 64 of the 122 parliamentary seats in July's elections, short of the two-thirds majority needed to form a government on its own\",\n",
       " \" Ranariddh and Sam Rainsy have charged that Hun Sen's victory in the elections was achieved through widespread fraud\",\n",
       " ' They have demanded a thorough investigation into their election complaints as a precondition for their cooperation in getting the national assembly moving and a new government formed',\n",
       " ' Hun Sen said on Friday that the opposition concerns over their safety in the country was ``just an excuse for them to stay abroad',\n",
       " \"'' Both Ranariddh and Sam Rainsy have been outside the country since parliament was ceremonially opened on Sep\",\n",
       " ' 24',\n",
       " \" Sam Rainsy and a number of opposition figures have been under court investigation for a grenade attack on Hun Sen's Phnom Penh residence on Sep\",\n",
       " ' 7',\n",
       " \" Hun Sen was not home at the time of the attack, which was followed by a police crackdown on demonstrators contesting Hun Sen's election victory\",\n",
       " \" The Sam Rainsy Party, in a statement released Friday, accused Hun Sen of being ``unwilling to make any compromise'' on negotiations to break the deadlock\",\n",
       " \" ``A meeting outside Cambodia, as suggested by the opposition, could place all parties on more equal footing,'' said the statement\",\n",
       " ' ``But the ruling party refuses to negotiate unless it is able to threaten its negotiating partners with arrest or worse',\n",
       " \"'' \"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = data.replace('\\n','').split('.')\n",
    "data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISF Scores: {'The': 0.4054651081081644, 'over': 1.0986122886681098, 'the': 0.4054651081081644, 'lazy': 1.0986122886681098, 'brown': 1.0986122886681098, 'fox': 1.0986122886681098, 'quick': 0.4054651081081644, 'jumps': 1.0986122886681098, 'dog.': 1.0986122886681098, 'jeopardize': 1.0986122886681098, 'A': 1.0986122886681098, 'will': 1.0986122886681098, 'enemy': 1.0986122886681098, 'gunboats.': 1.0986122886681098, 'movement': 1.0986122886681098, 'six': 1.0986122886681098, 'of': 1.0986122886681098, 'jump': 1.0986122886681098, 'five': 1.0986122886681098, 'boxing': 1.0986122886681098, 'quickly.': 1.0986122886681098, 'wizards': 1.0986122886681098}\n",
      "TF-ISF Vectors: [{'The': 0.4054651081081644, 'quick': 0.4054651081081644, 'brown': 1.0986122886681098, 'fox': 1.0986122886681098, 'jumps': 1.0986122886681098, 'over': 1.0986122886681098, 'the': 0.4054651081081644, 'lazy': 1.0986122886681098, 'dog.': 1.0986122886681098}, {'A': 1.0986122886681098, 'quick': 0.4054651081081644, 'movement': 1.0986122886681098, 'of': 1.0986122886681098, 'the': 0.4054651081081644, 'enemy': 1.0986122886681098, 'will': 1.0986122886681098, 'jeopardize': 1.0986122886681098, 'six': 1.0986122886681098, 'gunboats.': 1.0986122886681098}, {'The': 0.4054651081081644, 'five': 1.0986122886681098, 'boxing': 1.0986122886681098, 'wizards': 1.0986122886681098, 'jump': 1.0986122886681098, 'quickly.': 1.0986122886681098}]\n",
      "Cosine Similarities:\n",
      "Sentence 0 and 1: 0.0374\n",
      "Sentence 0 and 2: 0.0237\n",
      "Sentence 1 and 2: 0.0000\n",
      "TF-ISF Matrix:\n",
      "[[0.40546511 1.09861229 0.40546511 1.09861229 1.09861229 1.09861229\n",
      "  0.40546511 1.09861229 1.09861229 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.40546511 0.         0.         0.\n",
      "  0.40546511 0.         0.         1.09861229 1.09861229 1.09861229\n",
      "  1.09861229 1.09861229 1.09861229 1.09861229 1.09861229 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.40546511 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.09861229\n",
      "  1.09861229 1.09861229 1.09861229 1.09861229]]\n",
      "Terms: ['The', 'over', 'the', 'lazy', 'brown', 'fox', 'quick', 'jumps', 'dog.', 'jeopardize', 'A', 'will', 'enemy', 'gunboats.', 'movement', 'six', 'of', 'jump', 'five', 'boxing', 'quickly.', 'wizards']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def preprocess_text(sentences):\n",
    "    \"\"\"\n",
    "    Preprocess the sentences by removing stopwords and stemming.\n",
    "    \n",
    "    Parameters:\n",
    "        sentences (list of str): A list of sentences (strings).\n",
    "        \n",
    "    Returns:\n",
    "        list of list: A list of preprocessed terms for each sentence.\n",
    "    \"\"\"\n",
    "    # Load stopwords and initialize stemmer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        terms = [\n",
    "            stemmer.stem(word) for word in sentence.lower().split()\n",
    "            if word not in stop_words\n",
    "        ]\n",
    "        preprocessed_sentences.append(terms)\n",
    "    return preprocessed_sentences\n",
    "\n",
    "def calculate_isf_for_all_terms(sentences):\n",
    "    \"\"\"\n",
    "    Calculate the Inverse Sentence Frequency (ISF) for all terms in a list of sentences.\n",
    "    \n",
    "    Parameters:\n",
    "        sentences (list of str): A list of sentences (strings).\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are terms and values are their ISF scores.\n",
    "    \"\"\"\n",
    "    # Total number of sentences\n",
    "    Ns = len(sentences)\n",
    "    \n",
    "    # Dictionary to count the number of sentences containing each term\n",
    "    term_sentence_count = {}\n",
    "    \n",
    "    # Process sentences to count terms\n",
    "    for sentence in sentences:\n",
    "        # Extract unique terms from the sentence\n",
    "        terms = set(sentence.split())\n",
    "        for term in terms:\n",
    "            if term in term_sentence_count:\n",
    "                term_sentence_count[term] += 1\n",
    "            else:\n",
    "                term_sentence_count[term] = 1\n",
    "    \n",
    "    # Calculate ISF for each term\n",
    "    isf_dict = {}\n",
    "    for term, Nt_s in term_sentence_count.items():\n",
    "        isf_dict[term] = math.log(Ns / Nt_s)\n",
    "    \n",
    "    return isf_dict\n",
    "\n",
    "def calculate_tf_isf(sentences, isf_scores):\n",
    "    \"\"\"\n",
    "    Calculate the tf-isf vector for each sentence.\n",
    "    \n",
    "    Parameters:\n",
    "        sentences (list of str): A list of sentences (strings).\n",
    "        isf_scores (dict): A dictionary of ISF scores for terms.\n",
    "        \n",
    "    Returns:\n",
    "        list of dict: A list of dictionaries representing tf-isf vectors for each sentence.\n",
    "    \"\"\"\n",
    "    tf_isf_vectors = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        term_freq = defaultdict(int)\n",
    "        terms = sentence.split()  # Split the sentence into words\n",
    "        \n",
    "        # Count term frequencies\n",
    "        for term in terms:\n",
    "            term_freq[term] += 1\n",
    "        \n",
    "        # Calculate tf-isf\n",
    "        tf_isf = {term: term_freq[term] * isf_scores[term] for term in term_freq if term in isf_scores}\n",
    "        tf_isf_vectors.append(tf_isf)\n",
    "    \n",
    "    return tf_isf_vectors\n",
    "\n",
    "def calculate_tfisf_matrix(sentences, isf_dict):\n",
    "    \"\"\"\n",
    "    Calculate the TF-ISF matrix for a list of sentences.\n",
    "\n",
    "    Parameters:\n",
    "        sentences (list of str): A list of sentences (strings).\n",
    "        isf_dict (dict): Dictionary of ISF values for terms.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: TF-ISF matrix (rows: sentences, cols: terms).\n",
    "        list: List of terms corresponding to the columns of the matrix.\n",
    "    \"\"\"\n",
    "    # Extract all unique terms\n",
    "    terms = list(isf_dict.keys())\n",
    "    term_index = {term: idx for idx, term in enumerate(terms)}\n",
    "\n",
    "    # Initialize the TF-ISF matrix\n",
    "    tfisf_matrix = np.zeros((len(sentences), len(terms)))\n",
    "\n",
    "    # Calculate TF-ISF for each sentence\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Tokenize the sentence\n",
    "        words = sentence.split()\n",
    "        # Count term frequencies in the sentence\n",
    "        term_freq = Counter(words)\n",
    "\n",
    "        for term, freq in term_freq.items():\n",
    "            if term in term_index:\n",
    "                # Compute TF-ISF\n",
    "                tfisf_matrix[i, term_index[term]] = freq * isf_dict[term]\n",
    "\n",
    "    return tfisf_matrix, terms\n",
    "\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two tf-isf vectors.\n",
    "    \n",
    "    Parameters:\n",
    "        vec_a (dict): tf-isf vector of sentence A.\n",
    "        vec_b (dict): tf-isf vector of sentence B.\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity value.\n",
    "    \"\"\"\n",
    "    # Get the terms common to both vectors\n",
    "    common_terms = set(vec_a.keys()) & set(vec_b.keys())\n",
    "    \n",
    "    # Compute numerator\n",
    "    numerator = sum(vec_a[term] * vec_b[term] for term in common_terms)\n",
    "    \n",
    "    # Compute denominator\n",
    "    sum_a = sum(value**2 for value in vec_a.values())\n",
    "    sum_b = sum(value**2 for value in vec_b.values())\n",
    "    denominator = math.sqrt(sum_a) * math.sqrt(sum_b)\n",
    "    \n",
    "    return numerator / denominator if denominator != 0 else 0.0\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample sentences\n",
    "    sentences = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"A quick movement of the enemy will jeopardize six gunboats.\",\n",
    "        \"The five boxing wizards jump quickly.\"\n",
    "    ]\n",
    "    \n",
    "    # Preprocess sentences\n",
    "    #preprocessed_sentences = preprocess_text(sentences)\n",
    "    \n",
    "    # Calculate ISF\n",
    "    isf_scores = calculate_isf_for_all_terms(sentences)\n",
    "    print(\"ISF Scores:\", isf_scores)\n",
    "    \n",
    "    # Calculate tf-isf vectors\n",
    "    tf_isf_vectors = calculate_tf_isf(sentences, isf_scores)\n",
    "    print(\"TF-ISF Vectors:\", tf_isf_vectors)\n",
    "    \n",
    "    # Calculate cosine similarity between sentences\n",
    "    sim_01 = cosine_similarity(tf_isf_vectors[0], tf_isf_vectors[1])\n",
    "    sim_02 = cosine_similarity(tf_isf_vectors[0], tf_isf_vectors[2])\n",
    "    sim_12 = cosine_similarity(tf_isf_vectors[1], tf_isf_vectors[2])\n",
    "    \n",
    "    print(\"Cosine Similarities:\")\n",
    "    print(f\"Sentence 0 and 1: {sim_01:.4f}\")\n",
    "    print(f\"Sentence 0 and 2: {sim_02:.4f}\")\n",
    "    print(f\"Sentence 1 and 2: {sim_12:.4f}\")\n",
    "\n",
    "    # Calculate TF-ISF matrix\n",
    "    tfisf_matrix, terms = calculate_tfisf_matrix(sentences, isf_scores)\n",
    "\n",
    "    # Display results\n",
    "    print(\"TF-ISF Matrix:\")\n",
    "    print(tfisf_matrix)\n",
    "    print(\"Terms:\", terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Dissimilarity Scores:\n",
      "(The, over): 0.6309\n",
      "(The, the): 1.7095\n",
      "(The, lazy): 0.6309\n",
      "(The, brown): 0.6309\n",
      "(The, fox): 0.6309\n",
      "(The, quick): 1.7095\n",
      "(The, jumps): 0.6309\n",
      "(The, dog.): 0.6309\n",
      "(The, jump): 0.6309\n",
      "(The, five): 0.6309\n",
      "(The, boxing): 0.6309\n",
      "(The, quickly.): 0.6309\n",
      "(The, wizards): 0.6309\n",
      "(over, the): 0.6309\n",
      "(over, lazy): 0.0000\n",
      "(over, brown): 0.0000\n",
      "(over, fox): 0.0000\n",
      "(over, quick): 0.6309\n",
      "(over, jumps): 0.0000\n",
      "(over, dog.): 0.0000\n",
      "(the, lazy): 0.6309\n",
      "(the, brown): 0.6309\n",
      "(the, fox): 0.6309\n",
      "(the, quick): 0.0000\n",
      "(the, jumps): 0.6309\n",
      "(the, dog.): 0.6309\n",
      "(the, jeopardize): 0.6309\n",
      "(the, A): 0.6309\n",
      "(the, will): 0.6309\n",
      "(the, enemy): 0.6309\n",
      "(the, gunboats.): 0.6309\n",
      "(the, movement): 0.6309\n",
      "(the, six): 0.6309\n",
      "(the, of): 0.6309\n",
      "(lazy, brown): 0.0000\n",
      "(lazy, fox): 0.0000\n",
      "(lazy, quick): 0.6309\n",
      "(lazy, jumps): 0.0000\n",
      "(lazy, dog.): 0.0000\n",
      "(brown, fox): 0.0000\n",
      "(brown, quick): 0.6309\n",
      "(brown, jumps): 0.0000\n",
      "(brown, dog.): 0.0000\n",
      "(fox, quick): 0.6309\n",
      "(fox, jumps): 0.0000\n",
      "(fox, dog.): 0.0000\n",
      "(quick, jumps): 0.6309\n",
      "(quick, dog.): 0.6309\n",
      "(quick, jeopardize): 0.6309\n",
      "(quick, A): 0.6309\n",
      "(quick, will): 0.6309\n",
      "(quick, enemy): 0.6309\n",
      "(quick, gunboats.): 0.6309\n",
      "(quick, movement): 0.6309\n",
      "(quick, six): 0.6309\n",
      "(quick, of): 0.6309\n",
      "(jumps, dog.): 0.0000\n",
      "(jeopardize, A): 0.0000\n",
      "(jeopardize, will): 0.0000\n",
      "(jeopardize, enemy): 0.0000\n",
      "(jeopardize, gunboats.): 0.0000\n",
      "(jeopardize, movement): 0.0000\n",
      "(jeopardize, six): 0.0000\n",
      "(jeopardize, of): 0.0000\n",
      "(A, will): 0.0000\n",
      "(A, enemy): 0.0000\n",
      "(A, gunboats.): 0.0000\n",
      "(A, movement): 0.0000\n",
      "(A, six): 0.0000\n",
      "(A, of): 0.0000\n",
      "(will, enemy): 0.0000\n",
      "(will, gunboats.): 0.0000\n",
      "(will, movement): 0.0000\n",
      "(will, six): 0.0000\n",
      "(will, of): 0.0000\n",
      "(enemy, gunboats.): 0.0000\n",
      "(enemy, movement): 0.0000\n",
      "(enemy, six): 0.0000\n",
      "(enemy, of): 0.0000\n",
      "(gunboats., movement): 0.0000\n",
      "(gunboats., six): 0.0000\n",
      "(gunboats., of): 0.0000\n",
      "(movement, six): 0.0000\n",
      "(movement, of): 0.0000\n",
      "(six, of): 0.0000\n",
      "(jump, five): 0.0000\n",
      "(jump, boxing): 0.0000\n",
      "(jump, quickly.): 0.0000\n",
      "(jump, wizards): 0.0000\n",
      "(five, boxing): 0.0000\n",
      "(five, quickly.): 0.0000\n",
      "(five, wizards): 0.0000\n",
      "(boxing, quickly.): 0.0000\n",
      "(boxing, wizards): 0.0000\n",
      "(quickly., wizards): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from itertools import combinations\n",
    "\n",
    "def calculate_joint_isf(sentences, term_u, term_v):\n",
    "    \"\"\"\n",
    "    Calculate the joint ISF weight isf(u, v) for two terms.\n",
    "    \n",
    "    Parameters:\n",
    "        sentences (list of str): A list of sentences.\n",
    "        term_u (str): First term.\n",
    "        term_v (str): Second term.\n",
    "        \n",
    "    Returns:\n",
    "        float: The joint ISF weight isf(u, v).\n",
    "    \"\"\"\n",
    "    Ns = len(sentences)  # Total number of sentences\n",
    "    \n",
    "    # Count the number of sentences containing both terms\n",
    "    Nuv_s = sum(1 for sentence in sentences if term_u in sentence.split() and term_v in sentence.split())\n",
    "    \n",
    "    if Nuv_s == 0:\n",
    "        raise ValueError(f\"The terms '{term_u}' and '{term_v}' never co-occur in the corpus.\")\n",
    "    \n",
    "    return math.log(Ns / Nuv_s)\n",
    "\n",
    "\n",
    "def calculate_semantic_dissimilarity(sentences, isf_dict):\n",
    "    \"\"\"\n",
    "    Calculate the semantic dissimilarity (dsem) for all pairs of terms.\n",
    "    \n",
    "    Parameters:\n",
    "        sentences (list of str): A list of sentences.\n",
    "        isf_dict (dict): Dictionary of ISF values for individual terms.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are term pairs (u, v) and values are their semantic dissimilarities.\n",
    "    \"\"\"\n",
    "    terms = list(isf_dict.keys())\n",
    "    dsem_dict = {}\n",
    "    \n",
    "    for term_u, term_v in combinations(terms, 2):\n",
    "        try:\n",
    "            # Calculate joint ISF weight\n",
    "            joint_isf = calculate_joint_isf(sentences, term_u, term_v)\n",
    "            \n",
    "            # Calculate semantic dissimilarity\n",
    "            isf_u = isf_dict[term_u]\n",
    "            isf_v = isf_dict[term_v]\n",
    "            \n",
    "            min_isf = min(isf_u, isf_v)\n",
    "            max_isf = max(isf_u, isf_v)\n",
    "            \n",
    "            dsem = (joint_isf - min_isf) / max_isf\n",
    "            \n",
    "            # Store the result\n",
    "            dsem_dict[(term_u, term_v)] = dsem\n",
    "        except ValueError:\n",
    "            # Skip pairs that do not co-occur\n",
    "            continue\n",
    "    \n",
    "    return dsem_dict\n",
    "\n",
    "# Calculate ISF for all terms\n",
    "isf_dict = calculate_isf_for_all_terms(sentences)\n",
    "\n",
    "# Calculate semantic dissimilarity for all term pairs\n",
    "try:\n",
    "    dsem_scores = calculate_semantic_dissimilarity(sentences, isf_dict)\n",
    "    print(\"Semantic Dissimilarity Scores:\")\n",
    "    for (term_u, term_v), dsem in dsem_scores.items():\n",
    "        print(f\"({term_u}, {term_v}): {dsem:.4f}\")\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'The',\n",
       " 'boxing',\n",
       " 'brown',\n",
       " 'dog.',\n",
       " 'enemy',\n",
       " 'five',\n",
       " 'fox',\n",
       " 'gunboats.',\n",
       " 'jeopardize',\n",
       " 'jump',\n",
       " 'jumps',\n",
       " 'lazy',\n",
       " 'movement',\n",
       " 'of',\n",
       " 'over',\n",
       " 'quick',\n",
       " 'quickly.',\n",
       " 'six',\n",
       " 'the',\n",
       " 'will',\n",
       " 'wizards']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_terms(dissimilarity_scores):\n",
    "    \"\"\"\n",
    "    Extract unique terms from the keys of the dissimilarity scores dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        dissimilarity_scores (dict): Dictionary of dissimilarity scores, where keys are term pairs (u, v).\n",
    "\n",
    "    Returns:\n",
    "        list: Sorted list of unique terms.\n",
    "    \"\"\"\n",
    "    terms = set()\n",
    "    for term_u, term_v in dissimilarity_scores.keys():\n",
    "        terms.add(term_u)\n",
    "        terms.add(term_v)\n",
    "    return sorted(terms)\n",
    "\n",
    "unique_terms = extract_terms(dsem_scores)\n",
    "unique_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissimilarity Matrix:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.63092977 0.\n",
      "  0.         0.63092977 0.         0.        ]\n",
      " [0.         0.         0.63092977 0.63092977 0.63092977 0.\n",
      "  0.63092977 0.63092977 0.         0.         0.63092977 0.63092977\n",
      "  0.63092977 0.         0.         0.63092977 1.7095113  0.63092977\n",
      "  0.         1.7095113  0.         0.63092977]\n",
      " [0.         0.63092977 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.63092977 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.63092977 0.\n",
      "  0.         0.63092977 0.         0.        ]\n",
      " [0.         0.63092977 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.63092977 0.\n",
      "  0.         0.63092977 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.63092977 0.\n",
      "  0.         0.63092977 0.         0.        ]\n",
      " [0.         0.63092977 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.63092977 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.63092977 0.\n",
      "  0.         0.63092977 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.63092977 0.\n",
      "  0.         0.63092977 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.63092977 0.\n",
      "  0.         0.63092977 0.         0.        ]\n",
      " [0.         0.63092977 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.63092977 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.63092977 0.\n",
      "  0.         0.63092977 0.         0.        ]\n",
      " [0.         0.63092977 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.63092977 0.\n",
      "  0.         0.63092977 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.63092977 0.\n",
      "  0.         0.63092977 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.63092977 0.\n",
      "  0.         0.63092977 0.         0.        ]\n",
      " [0.         0.63092977 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.63092977 0.\n",
      "  0.         0.63092977 0.         0.        ]\n",
      " [0.63092977 1.7095113  0.         0.63092977 0.63092977 0.63092977\n",
      "  0.         0.63092977 0.63092977 0.63092977 0.         0.63092977\n",
      "  0.63092977 0.63092977 0.63092977 0.63092977 0.         0.\n",
      "  0.63092977 0.         0.63092977 0.        ]\n",
      " [0.         0.63092977 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.63092977 0.\n",
      "  0.         0.63092977 0.         0.        ]\n",
      " [0.63092977 1.7095113  0.         0.63092977 0.63092977 0.63092977\n",
      "  0.         0.63092977 0.63092977 0.63092977 0.         0.63092977\n",
      "  0.63092977 0.63092977 0.63092977 0.63092977 0.         0.\n",
      "  0.63092977 0.         0.63092977 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.63092977 0.\n",
      "  0.         0.63092977 0.         0.        ]\n",
      " [0.         0.63092977 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "def scores_to_dissimilarity_matrix(dissimilarity_scores, terms):\n",
    "    \"\"\"\n",
    "    Convert semantic dissimilarity scores to a dissimilarity matrix.\n",
    "\n",
    "    Parameters:\n",
    "        dissimilarity_scores (dict): Dictionary of dissimilarity scores, where keys are term pairs (u, v).\n",
    "        terms (list): List of unique terms.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Dissimilarity matrix (2D array).\n",
    "    \"\"\"\n",
    "    num_terms = len(terms)\n",
    "    term_index = {term: idx for idx, term in enumerate(terms)}\n",
    "\n",
    "    # Initialize a matrix with zeros\n",
    "    dissimilarity_matrix = np.zeros((num_terms, num_terms), dtype=np.float32)\n",
    "\n",
    "    # Populate the matrix with dissimilarity scores\n",
    "    for (term_u, term_v), score in dissimilarity_scores.items():\n",
    "        i, j = term_index[term_u], term_index[term_v]\n",
    "        dissimilarity_matrix[i, j] = score\n",
    "        dissimilarity_matrix[j, i] = score  # Ensure symmetry\n",
    "\n",
    "    return dissimilarity_matrix\n",
    "\n",
    "\n",
    "# Convert to dissimilarity matrix\n",
    "dissimilarity_matrix = scores_to_dissimilarity_matrix(dsem_scores, unique_terms)\n",
    "\n",
    "# Display the matrix\n",
    "print(\"Dissimilarity Matrix:\")\n",
    "print(dissimilarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1,  1,  2,  2,  0,  1,  2,  0,  0,  1,  2,  2,  0,  0,  2,  3,\n",
       "        1,  0,  3,  0,  1])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_dbscan(dissimilarity_matrix, epsilon, min_samples):\n",
    "    \"\"\"\n",
    "    Apply DBSCAN clustering on a precomputed dissimilarity matrix.\n",
    "\n",
    "    Parameters:\n",
    "        dissimilarity_matrix (np.ndarray): Square matrix of pairwise dissimilarities.\n",
    "        epsilon (float): The maximum distance between two samples for them to be in the same neighborhood.\n",
    "        min_samples (int): The number of samples in a neighborhood for a point to be considered as a core point.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Cluster labels for each term.\n",
    "    \"\"\"\n",
    "    # Apply DBSCAN with precomputed dissimilarity matrix\n",
    "    db = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "    results = db.fit(dissimilarity_matrix)\n",
    "    cluster_labels = results.labels_\n",
    "    return cluster_labels\n",
    "\n",
    "clusters = run_dbscan(dissimilarity_matrix, epsilon=0.00001, min_samples=2)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def associate_themes(topics, tfisf_matrix, delta):\n",
    "    \"\"\"\n",
    "    Associate themes to topics based on the SEMCOT algorithm and tagging rules.\n",
    "\n",
    "    Parameters:\n",
    "        topics (list or np.ndarray): Cluster labels for each term.\n",
    "        tfisf_matrix (np.ndarray): Matrix of `tf-isf` scores (rows: sentences, cols: terms).\n",
    "        delta (float): Threshold for tagging sentences with topics.\n",
    "\n",
    "    Returns:\n",
    "        dict: Themes, where each topic (key) maps to a set of sentence indices (values).\n",
    "    \"\"\"\n",
    "    # Number of sentences and terms\n",
    "    num_sentences, num_terms = tfisf_matrix.shape\n",
    "\n",
    "    # Get unique topics (exclude noise: -1)\n",
    "    unique_topics = set(topics) - {-1}\n",
    "\n",
    "    # Initialize themes\n",
    "    themes = {topic: set() for topic in unique_topics}\n",
    "\n",
    "    # Compute σ_il for each sentence and topic\n",
    "    for i in range(num_sentences):  # For each sentence\n",
    "        for topic in unique_topics:  # For each topic\n",
    "            # Calculate σ_il\n",
    "            sigma_il = sum(tfisf_matrix[i, j] for j in range(num_terms) if topics[j] == topic)\n",
    "            \n",
    "            # Tag the sentence with the topic if σ_il >= δ\n",
    "            if sigma_il >= delta:\n",
    "                themes[topic].add(i)\n",
    "\n",
    "    # Ensure each sentence is tagged with at least one topic\n",
    "    for i in range(num_sentences):\n",
    "        tagged = any(i in themes[topic] for topic in unique_topics)\n",
    "        if not tagged:\n",
    "            # Assign the sentence to the topic with the highest σ_il\n",
    "            best_topic = max(\n",
    "                unique_topics,\n",
    "                key=lambda topic: sum(tfisf_matrix[i, j] for j in range(num_terms) if topics[j] == topic)\n",
    "            )\n",
    "            themes[best_topic].add(i)\n",
    "\n",
    "    return themes\n",
    "themes = associate_themes(clusters, tfisf_matrix, delta=0.4) #Keys (0, 1, 2, etc.) represent topics (e.g., clusters of terms generated by SEMCOT or another clustering algorithm).Values (e.g., {0, 1, 2}) represent the indices of terms that belong to the corresponding topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prbably to bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0 -1  0 -1]\n",
      "Number of Topics: 2\n",
      "Cluster Labels: [-1 -1  0 -1  1]\n",
      "Final Epsilon: 0.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "def semcot_optimized(dsem_scores, isf_dict, epsilon_0=0.9, M=0.1, m=3, beta=0.95, mu=None):\n",
    "    \"\"\"\n",
    "    Optimized Semantic Clustering of Terms (SEMCOT) algorithm with minimum cluster size constraint.\n",
    "    \n",
    "    Parameters:\n",
    "        dsem_scores (dict): Dictionary of pairwise semantic dissimilarity scores.\n",
    "        isf_dict (dict): Dictionary of ISF values for terms.\n",
    "        epsilon_0 (float): Initial epsilon value for DBSCAN.\n",
    "        M (float): Maximum cluster size as a fraction of the total number of terms.\n",
    "        m (int): Minimum number of terms in a neighborhood for DBSCAN.\n",
    "        beta (float): Factor to reduce epsilon in each iteration.\n",
    "        mu (float): ISF threshold for including noisy terms as single-term topics.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Number of topics (K), topic labels (list of int), and final epsilon.\n",
    "    \"\"\"\n",
    "    # Number of terms (Nt)\n",
    "    terms = list(isf_dict.keys())\n",
    "    term_index = {term: idx for idx, term in enumerate(terms)}\n",
    "    Nt = len(terms)\n",
    "    \n",
    "    # Sparse matrix for dissimilarity scores\n",
    "    dsem_sparse = dok_matrix((Nt, Nt), dtype=np.float32)\n",
    "    for (term_u, term_v), score in dsem_scores.items():\n",
    "        i, j = term_index[term_u], term_index[term_v]\n",
    "        dsem_sparse[i, j] = score\n",
    "        dsem_sparse[j, i] = score  # Symmetry\n",
    "\n",
    "    # Replace missing entries with a large value\n",
    "    max_score = max(dsem_scores.values())\n",
    "    dsem_sparse = dsem_sparse.toarray()\n",
    "    dsem_sparse[dsem_sparse == 0] = max_score  # Handle missing pairs\n",
    "\n",
    "    # Initialize variables\n",
    "    epsilon = epsilon_0\n",
    "    max_cluster_size = int(M * Nt)\n",
    "    proceed = True\n",
    "    c = np.full(Nt, -1, dtype=int)  # Initialize cluster labels (-1 = noisy)\n",
    "    K = 0  # Number of clusters\n",
    "\n",
    "    \n",
    "    # Apply DBSCAN\n",
    "    db = DBSCAN(eps=epsilon, min_samples=m, metric=\"precomputed\")\n",
    "    c_temp = db.fit(dsem_sparse)\n",
    "    print(c_temp.labels_)\n",
    "        \n",
    "        # Count the maximum cluster size and filter clusters with one element\n",
    "        #cluster_sizes = {k: sum(c_temp == k) for k in set(c_temp) if k != -1}\n",
    "        #valid_clusters = {k for k, size in cluster_sizes.items() if size >= 2}\n",
    "\n",
    "        # Update labels, excluding single-element clusters\n",
    "        #c_temp_filtered = np.array([label if label in valid_clusters else -1 for label in c_temp])\n",
    "\n",
    "        #if max(cluster_sizes.values(), default=0) < max_cluster_size:\n",
    "        #    proceed = False\n",
    "        #    c = c_temp_filtered\n",
    "        #else:\n",
    "        #    epsilon *= beta  # Decrease epsilon\n",
    "\n",
    "    # Include noisy terms as single-term topics based on ISF threshold\n",
    "    if mu is not None:\n",
    "        for i, label in enumerate(c):\n",
    "            if label == -1 and isf_dict[terms[i]] >= mu:\n",
    "                c[i] = K\n",
    "                K += 1\n",
    "\n",
    "    return K, c, epsilon\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example ISF values for terms\n",
    "    isf_dict = isf_dict\n",
    "\n",
    "    # Example pairwise semantic dissimilarity scores\n",
    "    dsem_scores = dsem_scores\n",
    "\n",
    "    # Parameters\n",
    "    epsilon_0 = 0.3\n",
    "    M = 0.1  # 10% of terms\n",
    "    m = 2\n",
    "    beta = 0.95\n",
    "    mu = 1.0\n",
    "\n",
    "    # Run SEMCOT\n",
    "    K, c, epsilon_final = semcot_optimized(dsem_scores, isf_dict, epsilon_0, M, m, beta, mu)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Number of Topics: {K}\")\n",
    "    print(f\"Cluster Labels: {c}\")\n",
    "    print(f\"Final Epsilon: {epsilon_final}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Topics: 12\n",
      "Cluster Labels: [-1 -1 -1 -1 -1  0 -1  1  2  3  4 -1 -1  5  6  7  8  9 10 11]\n",
      "Final Epsilon: 0.4169621071437778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "def semcot_optimized(dsem_scores, isf_dict, epsilon_0=0.9, M=0.1, m=3, beta=0.95, mu=None):\n",
    "    \"\"\"\n",
    "    Optimized Semantic Clustering of Terms (SEMCOT) algorithm with minimum cluster size constraint.\n",
    "    \n",
    "    Parameters:\n",
    "        dsem_scores (dict): Dictionary of pairwise semantic dissimilarity scores.\n",
    "        isf_dict (dict): Dictionary of ISF values for terms.\n",
    "        epsilon_0 (float): Initial epsilon value for DBSCAN.\n",
    "        M (float): Maximum cluster size as a fraction of the total number of terms.\n",
    "        m (int): Minimum number of terms in a neighborhood for DBSCAN.\n",
    "        beta (float): Factor to reduce epsilon in each iteration.\n",
    "        mu (float): ISF threshold for including noisy terms as single-term topics.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Number of topics (K), topic labels (list of int), and final epsilon.\n",
    "    \"\"\"\n",
    "    # Number of terms (Nt)\n",
    "    terms = list(isf_dict.keys())\n",
    "    term_index = {term: idx for idx, term in enumerate(terms)}\n",
    "    Nt = len(terms)\n",
    "    \n",
    "    # Sparse matrix for dissimilarity scores\n",
    "    dsem_sparse = dok_matrix((Nt, Nt), dtype=np.float32)\n",
    "    for (term_u, term_v), score in dsem_scores.items():\n",
    "        i, j = term_index[term_u], term_index[term_v]\n",
    "        dsem_sparse[i, j] = score\n",
    "        dsem_sparse[j, i] = score  # Symmetry\n",
    "\n",
    "    # Replace missing entries with a large value\n",
    "    max_score = max(dsem_scores.values())\n",
    "    dsem_sparse = dsem_sparse.toarray()\n",
    "    dsem_sparse[dsem_sparse == 0] = max_score  # Handle missing pairs\n",
    "\n",
    "    # Initialize variables\n",
    "    epsilon = epsilon_0\n",
    "    max_cluster_size = int(M * Nt)\n",
    "    proceed = True\n",
    "    c = np.full(Nt, -1, dtype=int)  # Initialize cluster labels (-1 = noisy)\n",
    "    K = 0  # Number of clusters\n",
    "\n",
    "    while proceed:\n",
    "        # Apply DBSCAN\n",
    "        db = DBSCAN(eps=epsilon, min_samples=m, metric=\"precomputed\")\n",
    "        c_temp = db.fit_predict(dsem_sparse)\n",
    "        \n",
    "        # Count the maximum cluster size and filter clusters with one element\n",
    "        cluster_sizes = {k: sum(c_temp == k) for k in set(c_temp) if k != -1}\n",
    "        valid_clusters = {k for k, size in cluster_sizes.items() if size >= 2}\n",
    "\n",
    "        # Update labels, excluding single-element clusters\n",
    "        c_temp_filtered = np.array([label if label in valid_clusters else -1 for label in c_temp])\n",
    "\n",
    "        if max(cluster_sizes.values(), default=0) < max_cluster_size:\n",
    "            proceed = False\n",
    "            c = c_temp_filtered\n",
    "        else:\n",
    "            epsilon *= beta  # Decrease epsilon\n",
    "\n",
    "    # Include noisy terms as single-term topics based on ISF threshold\n",
    "    if mu is not None:\n",
    "        for i, label in enumerate(c):\n",
    "            if label == -1 and isf_dict[terms[i]] >= mu:\n",
    "                c[i] = K\n",
    "                K += 1\n",
    "\n",
    "    return K, c, epsilon\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example ISF values for terms\n",
    "    isf_dict = isf_dict\n",
    "\n",
    "    # Example pairwise semantic dissimilarity scores\n",
    "    dsem_scores = dsem_scores\n",
    "\n",
    "    # Parameters\n",
    "    epsilon_0 = 0.9\n",
    "    M = 0.1  # 10% of terms\n",
    "    m = 2\n",
    "    beta = 0.95\n",
    "    mu = 1.0\n",
    "\n",
    "    # Run SEMCOT\n",
    "    K, c, epsilon_final = semcot_optimized(dsem_scores, isf_dict, epsilon_0, M, m, beta, mu)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Number of Topics: {K}\")\n",
    "    print(f\"Cluster Labels: {c}\")\n",
    "    print(f\"Final Epsilon: {epsilon_final}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 84\u001b[0m\n\u001b[1;32m     77\u001b[0m dsem_scores \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     78\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm2\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;241m0.2\u001b[39m, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm3\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m     79\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm3\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;241m0.5\u001b[39m, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm4\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;241m0.3\u001b[39m,\n\u001b[1;32m     80\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm4\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;241m0.6\u001b[39m, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm5\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;241m0.4\u001b[39m\n\u001b[1;32m     81\u001b[0m }\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Parameters\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m K, c, epsilon_final \u001b[38;5;241m=\u001b[39m \u001b[43msemcot_optimized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsem_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misf_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\n\u001b[1;32m     86\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Results\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Topics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mK\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[68], line 22\u001b[0m, in \u001b[0;36msemcot_optimized\u001b[0;34m(dsem_scores, isf_dict, epsilon_0, M, m, beta, mu)\u001b[0m\n\u001b[1;32m     18\u001b[0m     dsem_sparse[j, i] \u001b[38;5;241m=\u001b[39m score  \u001b[38;5;66;03m# Ensure symmetry\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Inspect the dissimilarity matrix\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsem_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mviridis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m plt\u001b[38;5;241m.\u001b[39mcolorbar()\n\u001b[1;32m     24\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDissimilarity Matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.9/site-packages/matplotlib/pyplot.py:3562\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   3541\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   3542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   3543\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3560\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3561\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[0;32m-> 3562\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3566\u001b[0m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3567\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3568\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3571\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3577\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m     sci(__ret)\n\u001b[1;32m   3582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.9/site-packages/matplotlib/__init__.py:1473\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1478\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1479\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1480\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.9/site-packages/matplotlib/axes/_axes.py:5895\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5895\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5896\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.9/site-packages/matplotlib/image.py:729\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    728\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env/lib/python3.9/site-packages/matplotlib/image.py:692\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    690\u001b[0m A \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39msafe_masked_invalid(A, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcan_cast(A\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage data of dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    693\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverted to float\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    695\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def semcot_optimized(dsem_scores, isf_dict, epsilon_0=1.5, M=0.1, m=3, beta=0.9, mu=None):\n",
    "    \"\"\"\n",
    "    Optimized Semantic Clustering of Terms (SEMCOT) algorithm with improved filtering and validation.\n",
    "    \"\"\"\n",
    "    terms = list(isf_dict.keys())\n",
    "    term_index = {term: idx for idx, term in enumerate(terms)}\n",
    "    Nt = len(terms)\n",
    "\n",
    "    # Create a sparse matrix for dissimilarity scores\n",
    "    dsem_sparse = dok_matrix((Nt, Nt), dtype=np.float32)\n",
    "    for (term_u, term_v), score in dsem_scores.items():\n",
    "        i, j = term_index[term_u], term_index[term_v]\n",
    "        dsem_sparse[i, j] = score\n",
    "        dsem_sparse[j, i] = score  # Ensure symmetry\n",
    "\n",
    "\n",
    "    # Convert sparse matrix to a dense one and fill missing values\n",
    "    max_score = max(dsem_scores.values())\n",
    "    dsem_sparse = dsem_sparse.toarray()\n",
    "    dsem_sparse[dsem_sparse == 0] = max_score  # Replace missing entries with a large value\n",
    "\n",
    "    # Initialize variables\n",
    "    epsilon = epsilon_0\n",
    "    max_cluster_size = int(M * Nt)\n",
    "    proceed = True\n",
    "    c = np.full(Nt, -1, dtype=int)\n",
    "    K = 0\n",
    "\n",
    "    while proceed:\n",
    "        # Apply DBSCAN\n",
    "        db = DBSCAN(eps=epsilon, min_samples=m, metric=\"precomputed\")\n",
    "        c_temp = db.fit_predict(dsem_sparse)\n",
    "\n",
    "        # Filter out clusters with fewer than 2 elements\n",
    "        cluster_sizes = {k: sum(c_temp == k) for k in set(c_temp) if k != -1}\n",
    "        valid_clusters = {k for k, size in cluster_sizes.items() if size >= 2}\n",
    "\n",
    "        # Update labels to exclude invalid clusters\n",
    "        c_temp_filtered = np.array([label if label in valid_clusters else -1 for label in c_temp])\n",
    "\n",
    "        # Check if clusters satisfy max cluster size constraint\n",
    "        if max(cluster_sizes.values(), default=0) < max_cluster_size:\n",
    "            proceed = False\n",
    "            c = c_temp_filtered\n",
    "        else:\n",
    "            epsilon *= beta  # Decrease epsilon\n",
    "\n",
    "    # Include noisy terms as single-term topics based on ISF threshold\n",
    "    if mu is not None:\n",
    "        for i, label in enumerate(c):\n",
    "            if label == -1 and isf_dict[terms[i]] >= mu:\n",
    "                c[i] = K\n",
    "                K += 1\n",
    "\n",
    "    # Calculate the final number of clusters\n",
    "    K = len(set(c)) - (1 if -1 in c else 0)\n",
    "\n",
    "    return K, c, epsilon\n",
    "\n",
    "# Example ISF values\n",
    "isf_dict = {\n",
    "    'term1': 0.5, 'term2': 0.8, 'term3': 1.2, 'term4': 0.9, 'term5': 1.0\n",
    "}\n",
    "\n",
    "# Example dissimilarity scores\n",
    "dsem_scores = {\n",
    "    ('term1', 'term2'): 0.2, ('term1', 'term3'): 0.8,\n",
    "    ('term2', 'term3'): 0.5, ('term2', 'term4'): 0.3,\n",
    "    ('term3', 'term4'): 0.6, ('term4', 'term5'): 0.4\n",
    "}\n",
    "\n",
    "# Parameters\n",
    "K, c, epsilon_final = semcot_optimized(\n",
    "    dsem_scores, isf_dict, epsilon_0=1.5, M=0.2, m=2, beta=0.9, mu=0.7\n",
    ")\n",
    "\n",
    "# Results\n",
    "print(f\"Number of Topics: {K}\")\n",
    "print(f\"Cluster Labels: {c}\")\n",
    "print(f\"Final Epsilon: {epsilon_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SentenceHypergraph:\n",
    "    def __init__(self, sentences, themes, query, corpus):\n",
    "        \"\"\"\n",
    "        Initialize the sentence hypergraph.\n",
    "\n",
    "        Parameters:\n",
    "        - sentences: List of sentences (nodes).\n",
    "        - themes: Dictionary where keys are themes (hyperedges), and values are sets of sentence indices.\n",
    "        - query: Query string for relevance calculation.\n",
    "        - corpus: Entire corpus for centrality calculation.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.themes = themes\n",
    "        self.query = query\n",
    "        self.corpus = corpus\n",
    "\n",
    "        # Node weights (sentence lengths)\n",
    "        self.node_weights = np.array([len(sentence.split()) for sentence in sentences])\n",
    "\n",
    "        # Hyperedges and their weights\n",
    "        self.hyperedges = [set(sentences) for sentences in themes.values()]\n",
    "        self.hyperedge_weights = self.compute_hyperedge_weights()\n",
    "\n",
    "        # Incidence lists\n",
    "        self.incidence_lists = self.compute_incidence_lists()\n",
    "\n",
    "    def compute_hyperedge_weights(self, lambda_param=0.5):\n",
    "        \"\"\"\n",
    "        Compute weights for hyperedges based on centrality and query relevance.\n",
    "\n",
    "        Parameters:\n",
    "        - lambda_param: Weighting parameter for centrality and query relevance.\n",
    "\n",
    "        Returns:\n",
    "        - List of hyperedge weights.\n",
    "        \"\"\"\n",
    "        weights = []\n",
    "        for theme_sentences in self.hyperedges:\n",
    "            theme_text = \" \".join([self.sentences[idx] for idx in theme_sentences])\n",
    "            \n",
    "            # Centrality: Similarity with the corpus\n",
    "            centrality = self.compute_similarity(theme_text, self.corpus)\n",
    "            \n",
    "            # Relevance: Similarity with the query\n",
    "            relevance = self.compute_similarity(theme_text, self.query)\n",
    "            \n",
    "            # Hyperedge weight\n",
    "            weight = (1 - lambda_param) * centrality + lambda_param * relevance\n",
    "            weights.append(weight)\n",
    "        \n",
    "        return np.array(weights)\n",
    "\n",
    "    def compute_incidence_lists(self):\n",
    "        \"\"\"\n",
    "        Compute incidence lists for nodes (sentences).\n",
    "\n",
    "        Returns:\n",
    "        - List of lists where each entry corresponds to the hyperedges incident on a node.\n",
    "        \"\"\"\n",
    "        incidence = [[] for _ in range(len(self.sentences))]\n",
    "        for e_idx, hyperedge in enumerate(self.hyperedges):\n",
    "            for node in hyperedge:\n",
    "                incidence[node].append(e_idx)\n",
    "        return incidence\n",
    "\n",
    "    def compute_similarity(self, text1, text2):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two texts based on TF-ISF.\n",
    "\n",
    "        Parameters:\n",
    "        - text1: First text.\n",
    "        - text2: Second text.\n",
    "\n",
    "        Returns:\n",
    "        - Cosine similarity.\n",
    "        \"\"\"\n",
    "        # Tokenize and compute term frequencies\n",
    "        tf1 = self.compute_tf(text1)\n",
    "        tf2 = self.compute_tf(text2)\n",
    "        \n",
    "        # Compute dot product and norms\n",
    "        dot_product = sum(tf1[term] * tf2.get(term, 0) for term in tf1)\n",
    "        norm1 = np.sqrt(sum(val ** 2 for val in tf1.values()))\n",
    "        norm2 = np.sqrt(sum(val ** 2 for val in tf2.values()))\n",
    "\n",
    "        return dot_product / (norm1 * norm2) if norm1 and norm2 else 0.0\n",
    "\n",
    "    def compute_tf(self, text):\n",
    "        \"\"\"\n",
    "        Compute term frequencies for a given text.\n",
    "\n",
    "        Parameters:\n",
    "        - text: Input text.\n",
    "\n",
    "        Returns:\n",
    "        - Dictionary of term frequencies.\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        tf = {}\n",
    "        for word in words:\n",
    "            tf[word] = tf.get(word, 0) + 1\n",
    "        return tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tl_transum(hypergraph, target_length):\n",
    "    \"\"\"\n",
    "    Transversal Summarization with Target Length (TL-TranSum).\n",
    "\n",
    "    Parameters:\n",
    "        hypergraph (SentenceHypergraph): The sentence hypergraph.\n",
    "        target_length (int): Maximum length of the summary (in words).\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of sentences to include in the summary.\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    sentences = list(range(len(hypergraph.sentences)))\n",
    "    node_weights = hypergraph.node_weights\n",
    "    hyperedge_weights = hypergraph.hyperedge_weights\n",
    "    incidence_lists = hypergraph.incidence_lists\n",
    "\n",
    "    # Compute initial scores for each sentence\n",
    "    scores = []\n",
    "    for i in sentences:\n",
    "        coverage = sum(hyperedge_weights[e] for e in incidence_lists[i])\n",
    "        scores.append(coverage / node_weights[i])  # ri = 1/φi * ∑ w_e\n",
    "\n",
    "    # Select sentences\n",
    "    summary = set()\n",
    "    total_length = 0\n",
    "\n",
    "    while total_length < target_length:\n",
    "        # Select the sentence with the highest score\n",
    "        best_sentence = max(range(len(scores)), key=lambda i: scores[i])\n",
    "        summary.add(best_sentence)\n",
    "\n",
    "        # Update total length\n",
    "        total_length += node_weights[best_sentence]\n",
    "\n",
    "        # Update scores\n",
    "        for i in sentences:\n",
    "            if i not in summary:\n",
    "                overlap = sum(hyperedge_weights[e] for e in incidence_lists[i] if e in incidence_lists[best_sentence])\n",
    "                scores[i] -= overlap / node_weights[i]\n",
    "\n",
    "        # Remove the selected sentence from the score computation\n",
    "        scores[best_sentence] = -1\n",
    "\n",
    "        # Break if all sentences are selected\n",
    "        if total_length > target_length:\n",
    "            break\n",
    "\n",
    "    return list(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: ['The five boxing wizards jump quickly.']\n"
     ]
    }
   ],
   "source": [
    "query = \"lazy\"\n",
    "corpus = \" \".join(sentences)\n",
    "target_length = 4\n",
    "\n",
    "# Construct hypergraph\n",
    "hypergraph = SentenceHypergraph(sentences, themes, query, corpus)\n",
    "\n",
    "# Generate summary using TL-TranSum\n",
    "summary_indices = tl_transum(hypergraph, target_length)\n",
    "\n",
    "# Display the summary\n",
    "summary = [sentences[i] for i in summary_indices]\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumps over the lazy dog.',\n",
       " 'A quick movement of the enemy will jeopardize six gunboats.',\n",
       " 'The five boxing wizards jump quickly.']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0, 1, 2}, 1: {0, 1, 2}, 2: {0, 1}, 3: {1, 2}}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "themes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
